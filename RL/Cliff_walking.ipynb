{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       "  4\n",
       " 12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# world height\n",
    "WORLD_HEIGHT = 4\n",
    "\n",
    "# world width\n",
    "WORLD_WIDTH = 12\n",
    "\n",
    "# probability for exploration\n",
    "EPSILON = 0.5\n",
    "\n",
    "# step size\n",
    "ALPHA = 0.5\n",
    "\n",
    "# gamma for Q-Learning and Expected Sarsa\n",
    "GAMMA = 1\n",
    "\n",
    "# all possible actions\n",
    "ACTION_UP = 1\n",
    "ACTION_DOWN = 2\n",
    "ACTION_LEFT = 3\n",
    "ACTION_RIGHT = 4\n",
    "ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
    "\n",
    "# initial state action pair values\n",
    "START = [4, 1]\n",
    "GOAL = [4, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function step(state, action)\n",
    "    i, j = state\n",
    "    if action == ACTION_UP\n",
    "        next_state = [max(i - 1, 1), j]\n",
    "    elseif action == ACTION_LEFT\n",
    "        next_state = [i, max(j - 1, 1)]\n",
    "    elseif action == ACTION_RIGHT\n",
    "        next_state = [i, min(j + 1, WORLD_WIDTH)]\n",
    "    elseif action == ACTION_DOWN\n",
    "        next_state = [min(i + 1, WORLD_HEIGHT), j]\n",
    "    else\n",
    "        return false\n",
    "    end\n",
    "    \n",
    "    reward = -1\n",
    "    if (action == ACTION_DOWN && i == 3 && 2 <= j && j <= 11) ||\n",
    "        (action == ACTION_RIGHT && state == START)\n",
    "        reward = -100\n",
    "        next_state = START\n",
    "    end\n",
    "    return next_state, reward\n",
    "end\n",
    "\n",
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choose_action (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function choose_action(state, q_value)\n",
    "    if rand(1)[1] < EPSILON\n",
    "        return ACTIONS[rand(1:4)]\n",
    "    else\n",
    "        values_ = q_value[state[1], state[2],:]\n",
    "        action = []\n",
    "        for i in enumerate(values_)\n",
    "            action_,value_ = i\n",
    "            if value_ == maximum(values_)\n",
    "                push!(action,action_)\n",
    "            end\n",
    "        end\n",
    "        action = action[rand(1:length(action))]\n",
    "        return action\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sarsa (generic function with 3 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sarsa(q_value, expected = false, step_size = ALPHA)\n",
    "    state = START\n",
    "    action = choose_action(state, q_value)\n",
    "    rewards = 0.0\n",
    "    while state != GOAL\n",
    "        next_state, reward = step(state, action)\n",
    "        next_action = choose_action(next_state,q_value)\n",
    "        rewards += reward\n",
    "        if !expected\n",
    "            target = q_value[next_state[1], next_state[2], next_action]\n",
    "        else\n",
    "            target = 0.0\n",
    "            q_next = q_value[next_state[1], next_state[2], :]\n",
    "            best_actions = transpose(hcat(nonzero(q_next == np.max(q_next))...))\n",
    "            for action_ in ACTIONS\n",
    "                if action_ in best_actions\n",
    "                    target += ((1.0 - EPSILON) / len(best_actions) + EPSILON / len(ACTIONS)) * q_value[next_state[1], next_state[2], action_]\n",
    "                else\n",
    "                    target += EPSILON / len(ACTIONS) * q_value[next_state[1], next_state[2], action_]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        target = target * GAMMA\n",
    "        q_value[state[1], state[2], action] += step_size * (reward + target - q_value[state[1], state[2], action])\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    end\n",
    "    return rewards\n",
    "end            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q_learning (generic function with 2 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function q_learning(q_value, step_size = ALPHA)\n",
    "    state = START\n",
    "    rewards = 0.0\n",
    "    while state != GOAL\n",
    "        action = choose_action(state, q_value)\n",
    "        next_state, reward = step(state, action)\n",
    "        rewards += reward\n",
    "        # Q-Learning update\n",
    "        q_value[state[1], state[2], action] += step_size * (\n",
    "                reward + GAMMA * maximum(q_value[next_state[1], next_state[2], :]) -\n",
    "                q_value[state[1], state[2], action])\n",
    "        state = next_state\n",
    "    end\n",
    "    return rewards\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "print_optimal_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FUNCION PARA IMPRIMIR POLITICA ROBADA ILEGALMENTE DEL RICARDO\n",
    "function print_optimal_policy(q_value)\n",
    "    optimal_policy = []\n",
    "    for i in range(1, stop=WORLD_HEIGHT)\n",
    "        push!(optimal_policy,[])\n",
    "        for j in range(1, stop=WORLD_WIDTH)\n",
    "            if [i, j] == GOAL\n",
    "                append!(optimal_policy[end], 'G')\n",
    "                continue\n",
    "            end\n",
    "            bestAction = argmax(q_value[i, j, :])\n",
    "            if bestAction == ACTION_UP\n",
    "                append!(optimal_policy[end], 'U')\n",
    "            elseif bestAction == ACTION_DOWN\n",
    "                append!(optimal_policy[end], 'D')\n",
    "            elseif bestAction == ACTION_LEFT\n",
    "                append!(optimal_policy[end], 'L')\n",
    "            elseif bestAction == ACTION_RIGHT\n",
    "                append!(optimal_policy[end], 'R')\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    for row in optimal_policy\n",
    "        println(row)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episodes = 500\n",
    "runs = 500\n",
    "rewards_q_learning = zeros(episodes)\n",
    "rewards_sarsa = zeros(episodes)\n",
    "q_q_learning = []\n",
    "q_sarsa = []\n",
    "for r in 1:runs  \n",
    "    q_q_learning = zeros(WORLD_HEIGHT, WORLD_WIDTH , 4)\n",
    "    q_sarsa = zeros(WORLD_HEIGHT, WORLD_WIDTH, 4)\n",
    "    for i in 1:episodes\n",
    "        rewards_q_learning[i] += q_learning(q_q_learning)\n",
    "        rewards_sarsa[i] += sarsa(q_sarsa)\n",
    "    end\n",
    "end\n",
    "\n",
    "rewards_q_learning /= runs\n",
    "rewards_sarsa /= runs\n",
    "\n",
    "println(\"Q-Learning Optimal Policy:\")\n",
    "print_optimal_policy(q_q_learning)\n",
    "println(\"SARSA Optimal Policy:\")\n",
    "print_optimal_policy(q_sarsa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw reward curves\n",
    "Plots.plot(rewards_sarsa, label=\"Sarsa\", fmt = :png)\n",
    "Plots.plot!(rewards_q_learning, label=\"Q-Learning\")\n",
    "Plots.xaxis!(\"Episodes\")\n",
    "Plots.yaxis!(\"Sum of rewards during episode\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
