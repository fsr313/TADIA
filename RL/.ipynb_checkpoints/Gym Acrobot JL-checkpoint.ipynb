{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACROBOT\n",
    "---\n",
    "Acrobot es un pequeno robot conformado por dos brazos creando un pendulo. El objetivo del robot es que pase una altura predefinida, solamente utilizando 3 acciones distintas, Empujar a la derecha, Empujar a la izquierda y no hacer nada, con estas acciones es suficiente para que el robot empiece a columpearse y tomar velocidad rotacional para subir hasta la linea objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall\n",
    "using PyPlot\n",
    "ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# librerias\n",
    "---\n",
    "Utilizaremos librerias de python, ya que el sistema operativo que utilizo no permite utilizar OpenAIGym con Julia, esto reducira dramaticamente nuestra eficiencia y probablemente haga el entrenamiento mas lento y menos eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'matplotlib.patches.Rectangle'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@pyimport numpy as np\n",
    "@pyimport gym\n",
    "\n",
    "Rectangle = pyimport(\"matplotlib.patches\")[\"Rectangle\"]\n",
    "\n",
    "plt[:style][:use](\"ggplot\")\n",
    "Rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializar environment\n",
    "---\n",
    "rapidamente creamos nuestro environment del acrobot y aprovecharemos la oportunidad para imprimir cual es valor mas alto que toman sus estados, el mas bajo y el numero de acciones que hay. Esto sera util mas adelante para poder crear nuestra tabla q y definir nuestras acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: PyObject Box(6,)\n",
      "-  low: Float32[-1.0, -1.0, -1.0, -1.0, -12.5664, -28.2743]\n",
      "- high: Float32[1.0, 1.0, 1.0, 1.0, 12.5664, 28.2743]\n",
      "Action space: PyObject Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    " \n",
    "env[:seed](505);\n",
    "println(\"State space: \", env[:observation_space])\n",
    "\n",
    "println(\"-  low: \", env[:observation_space][:low])\n",
    "\n",
    "println(\"- high: \", env[:observation_space][:high])\n",
    "println(\"Action space: \", env[:action_space])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crear un tiling grid\n",
    "---\n",
    "A un tiling le llamamos un lugar en donde podremos encontrar un conjunto infinito de estados ya que estos estan en un espacio continuo.\n",
    "Esta funcion es bastante sencilla, dados n lows, n highs, n tamanos do bins y n offsets, regresamos arreglos del tamano de los bins haciendo divisiones uniformes entre low[n] y high[n] con el offset[n] agregado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.0, 1.0]/10 + (-0.1) => [-0.9, -0.7, -0.5, -0.3, -0.1, 0.1, 0.3, 0.5, 0.7]\n",
      "    [-5.0, 5.0]/10 + (0.5) => [-3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5]\n"
     ]
    }
   ],
   "source": [
    "function create_tiling_grid(low, high, bins = (10, 10), offsets = (0.0, 0.0))\n",
    "\n",
    "    \n",
    "    grid = [collect(range(low[dim], stop = high[dim], length = bins[dim] + 1))[2:end-1] for dim in 1:length(bins)]\n",
    "\n",
    "    for i in 1:length(bins)\n",
    "        for j in 1:bins[1]-1\n",
    "            grid[i][j] = grid[i][j] + offsets[i]\n",
    "        end\n",
    "    end\n",
    "    println(\"Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\")\n",
    "    for (l, h, b, o, splits) in zip(low, high, bins, offsets, grid)\n",
    "        println(\"    [$l, $h]/$b + ($o) => $splits\")\n",
    "    end\n",
    "    return grid\n",
    "end\n",
    "low = [-1.0, -5.0]\n",
    "high = [1.0, 5.0]\n",
    "\n",
    "create_tiling_grid(low, high, (10, 10), (-0.1, 0.5));  # [test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crear tilings\n",
    "---\n",
    "para crear tilings simplemente hacemos varios grids, aqui abajo podemos ver como funciona la funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.0, 1.0]/10 + (-0.066) => [-0.866, -0.666, -0.466, -0.266, -0.066, 0.134, 0.334, 0.534, 0.734]\n",
      "    [-5.0, 5.0]/10 + (-0.33) => [-4.33, -3.33, -2.33, -1.33, -0.33, 0.67, 1.67, 2.67, 3.67]\n",
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.0, 1.0]/10 + (0.0) => [-0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8]\n",
      "    [-5.0, 5.0]/10 + (0.0) => [-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0]\n",
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.0, 1.0]/10 + (0.066) => [-0.734, -0.534, -0.334, -0.134, 0.066, 0.266, 0.466, 0.666, 0.866]\n",
      "    [-5.0, 5.0]/10 + (0.33) => [-3.67, -2.67, -1.67, -0.67, 0.33, 1.33, 2.33, 3.33, 4.33]\n"
     ]
    }
   ],
   "source": [
    "function create_tilings(low, high, tiling_specs)\n",
    "    low = convert(Array{Float64,1}, low)\n",
    "    high = convert(Array{Float64,1}, high)\n",
    "    return[create_tiling_grid(low,high,aux[1],aux[2]) for aux in tiling_specs]\n",
    "end\n",
    "tiling_specs = [((10, 10), (-0.066, -0.33)),\n",
    "                ((10, 10), (0.0, 0.0)),\n",
    "                ((10, 10), (0.066, 0.33))]\n",
    "tilings = create_tilings(low, high, tiling_specs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretizacion\n",
    "---\n",
    "Aqui utilizamos dos sencillas funciones la primera que es Discretize, discretiza utilizando una malla dada, el numero discretizado es de las dimensiones del mismo sample enviado\n",
    "\n",
    "---\n",
    "# Codificacion\n",
    "---\n",
    "Utilizando nuestra discretizacion podemos acomodar el tile en donde cada una de nuestros datos quedara, a esta parte es la que llamamos tile-coding, porque codificamos las muestras en los tiles donde deberian quedando, esto usando vectores de tuplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Any,1}:\n",
       " Any[Any[1, 1], Any[1, 1], Any[1, 1]]      \n",
       " Any[Any[2, 9], Any[2, 9], Any[1, 8]]      \n",
       " Any[Any[3, 6], Any[3, 5], Any[3, 5]]      \n",
       " Any[Any[7, 4], Any[7, 4], Any[6, 3]]      \n",
       " Any[Any[7, 4], Any[6, 4], Any[6, 3]]      \n",
       " Any[Any[10, 8], Any[9, 8], Any[9, 8]]     \n",
       " Any[Any[9, 2], Any[9, 2], Any[9, 1]]      \n",
       " Any[Any[10, 10], Any[10, 10], Any[10, 10]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function discretize(sample, grid)\n",
    "    list = []\n",
    "    for a in zip(sample,grid)\n",
    "        aux = searchsortedfirst(a[2],a[1])\n",
    "        push!(list,aux)\n",
    "    end\n",
    "\n",
    "    return list\n",
    "end\n",
    "function tile_encode(sample, tilings)\n",
    "    encoded_sample = []\n",
    "    for grid in tilings\n",
    "        aux = discretize(sample,grid)\n",
    "        push!(encoded_sample,aux)\n",
    "    end\n",
    "    \n",
    "    return encoded_sample\n",
    "end\n",
    "samples = [(-1.2 , -5.1 ),\n",
    "           (-0.75,  3.25),\n",
    "           (-0.5 ,  0.0 ),\n",
    "           ( 0.25, -1.9 ),\n",
    "           ( 0.15, -1.75),\n",
    "           ( 0.75,  2.5 ),\n",
    "           ( 0.7 , -3.7 ),\n",
    "           ( 1.0 ,  5.0 )]\n",
    "encoded_samples = []\n",
    "for sample in samples\n",
    "    aux = tile_encode(sample,tilings)\n",
    "    push!(encoded_samples,aux)\n",
    "end\n",
    "encoded_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTable\n",
    "---\n",
    "Esta estructura es bastante sencilla, y se explica practicamente sola, solamente recibe el tamano de los estados, de las acciones y crea una tabla de zeros del tamano de state_size + action_size. Exactamente lo mismo que ya hemos hecho en pasados programas, pero esta vez utilizamos estructuras,\n",
    "\n",
    "---\n",
    "# TiledQTable\n",
    "---\n",
    "Esta estructura y funciones son un intermedio de comunicacion, con el objetivo de poder ver donde esta vada uno de los valores de la tabla q, podemos notar que en esta estructura ahora creamos tiles, las cuales utilizaremos para poder acceder a la tabla utilizando el tile_encode.\n",
    "Junto con esto creamos dos funciones muy utiles que son getTiledQTable y updateTiledQTable, ambas fetchean las tablas q y un estado para poder saber que accion tomar y para actualizar su valor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.0, 1.0]/10 + (-0.066) => [-0.866, -0.666, -0.466, -0.266, -0.066, 0.134, 0.334, 0.534, 0.734]\n",
      "    [-5.0, 5.0]/10 + (-0.33) => [-4.33, -3.33, -2.33, -1.33, -0.33, 0.67, 1.67, 2.67, 3.67]\n",
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.0, 1.0]/10 + (0.0) => [-0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8]\n",
      "    [-5.0, 5.0]/10 + (0.0) => [-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0]\n",
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.0, 1.0]/10 + (0.066) => [-0.734, -0.534, -0.334, -0.134, 0.066, 0.266, 0.466, 0.666, 0.866]\n",
      "    [-5.0, 5.0]/10 + (0.33) => [-3.67, -2.67, -1.67, -0.67, 0.33, 1.33, 2.33, 3.33, 4.33]\n",
      "QTable(): size = (10, 10, 2)\n",
      "QTable(): size = (10, 10, 2)\n",
      "QTable(): size = (10, 10, 2)\n",
      "TiledQTable(): no. of internal tables = 3\n",
      "[GET]    Q( (0.25, -1.9) , 1) = 0.0\n",
      "[UPDATE] Q( (0.15, -1.75) , 1) = 1.0\n",
      "[GET]    Q( (0.25, -1.9) , 1) = 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "mutable struct QTable\n",
    "    state_size\n",
    "    action_size\n",
    "    q_table\n",
    "    function QTable(state_size,action_size)\n",
    "        println(\"QTable(): size = \", tuple(state_size..., action_size...))\n",
    "        new(state_size, action_size, zeros(tuple(state_size..., action_size...))) \n",
    "    end\n",
    "end\n",
    "    \n",
    "mutable struct TiledQTable\n",
    "    tilings\n",
    "    state_sizes\n",
    "    action_size\n",
    "    q_tables\n",
    "    function TiledQTable(low, high, tiling_specs, action_size)\n",
    "        tilings = create_tilings(low, high, tiling_specs)\n",
    "        state_sizes = [tuple([length(splits)+1 for splits in tiling_grid]...) for tiling_grid in tilings]\n",
    "        action_size = action_size\n",
    "        q_tables = [QTable(state_size,action_size) for state_size in state_sizes]\n",
    "        println(\"TiledQTable(): no. of internal tables = $(length(q_tables))\")\n",
    "        new(tilings, state_sizes, action_size, q_tables)\n",
    "    end\n",
    "end\n",
    "\n",
    "function getTiledQTable(self, state, action)\n",
    "    encoded_state = tile_encode(state, self.tilings)\n",
    "    value = 0.0\n",
    "    for (idx, q_table) in zip(encoded_state, self.q_tables)\n",
    "        value = value + q_table.q_table[idx..., action]\n",
    "    end\n",
    "    return value/length(self.q_tables)\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function updateTiledQTable(self::TiledQTable, state, action, value, alpha = 0.1)\n",
    "    encoded_state = tile_encode(state, self.tilings)\n",
    "    alphacomp = 1 - alpha\n",
    "    \n",
    "    for (idx,q_table) in zip(encoded_state, self.q_tables)\n",
    "        auxval = q_table.q_table[idx..., action]\n",
    "        q_table.q_table[idx...,action] = (alpha * value) + (alphacomp) * auxval\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "#\n",
    "tq = TiledQTable(low, high, tiling_specs, 2);\n",
    "#println(\"#################################################################\")\n",
    "\n",
    "s1 = 4; s2 = 5; a = 1; q = 1.0\n",
    "\n",
    "\n",
    "\n",
    "println(\"[GET]    Q( $(samples[s1]) , $a) = $(getTiledQTable(tq, samples[s1], a))\") # check value at sample = s1\n",
    "println(\"[UPDATE] Q( $(samples[s2]) , $a) = $q\") \n",
    "updateTiledQTable(tq,samples[s2], a, q)  # update value for sample with some common tile(s)\n",
    "println(\"[GET]    Q( $(samples[s1]) , $a) = $(getTiledQTable(tq, samples[s1], a))\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QlearningAgent\n",
    "---\n",
    "aqui se acaban nuestras anomalias, no hemos de utilizar nada que no hayamos utilizado anteriormente, pero igual definimos nuestro agente que aprendera, su definicion es bastante sencilal nuestro agente tendra una tabla q, por esto tambien sabra el tamano de los estados y acciones, factor de descuento, probabilidad de exploracion, tasa de aprendizaje, el ultimo estado y accion que han ocurrido en el aprendizaje.\n",
    "nuestras funcioncillas\n",
    "empezando por reset_episode, literalmente es lo que dice cada que es llamada resetea nuestro env\n",
    "y act, la cual es nuestra funcion que en otros lugares llamamos Q-learning, lo que esta hace es aplicar q-learning si es que el parametro do modo es de entrenamiento, si no es asi regresara una accion greedy dado un estado.\n",
    "Estas funciones y estructura estan generalizadas, podemos utilizarlas para otros problemas, no son unicas de este.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "act (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct QLearningAgent\n",
    "    env\n",
    "    tq\n",
    "    state_sizes\n",
    "    action_size\n",
    "    alpha\n",
    "    gamma\n",
    "    epsilon\n",
    "    epsilon_decay_rate\n",
    "    min_epsilon\n",
    "    initial_epsilon\n",
    "    last_state\n",
    "    last_action\n",
    "    function QLearningAgent(env, tq, alpha = 0.02, gamma = 0.99,\n",
    "        epsilon = 1.0, epsilon_decay_rate = 1, min_epsilon = 0.01)\n",
    "        \n",
    "        # Environment info\n",
    "        env = env\n",
    "        tq = tq\n",
    "        state_sizes = tq.state_sizes\n",
    "        action_size = env[:action_space][:n]\n",
    "        \n",
    "        # Learning parameters\n",
    "        alpha = alpha\n",
    "        gamma = gamma\n",
    "        epsilon = epsilon\n",
    "        initial_epsilon = epsilon\n",
    "        min_epsilon = min_epsilon\n",
    "        \n",
    "        println(\"Environment: \", env)\n",
    "        println(\"State space sizes: \", state_sizes)\n",
    "        println(\"Action space size: \", action_size)\n",
    "        new(env, tq, state_sizes, action_size, alpha, gamma, epsilon, epsilon_decay_rate, min_epsilon, initial_epsilon,nothing, nothing)\n",
    "    end\n",
    "end\n",
    "function reset_episode(self::QLearningAgent, state)\n",
    "    #reducir gradualmente la exploracion\n",
    "    self.epsilon = self.epsilon * self.epsilon_decay_rate\n",
    "    self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "    \n",
    "    self.last_state = state\n",
    "    Q_s = [getTiledQTable(self.tq, state, action) for action in 1:length(self.action_size)]\n",
    "    self.last_action = np.argmax(Q_s) + 1\n",
    "    return self.last_action  \n",
    "end\n",
    "\n",
    "    \n",
    "\n",
    "function act(self::QLearningAgent, state, reward, mode)\n",
    "    Q_s = [getTiledQTable(self.tq, state, action) for action in 1:self.action_size]\n",
    "    greedy_action = np.argmax(Q_s) + 1\n",
    "    \n",
    "    \n",
    "    if mode == \"test\"\n",
    "        action = greedy_action\n",
    "    else\n",
    "        value = reward + self.gamma * maximum(Q_s)\n",
    "        updateTiledQTable(self.tq, self.last_state, self.last_action, value, self.alpha)\n",
    "        \n",
    "        do_exploration = np.random[:uniform](0,1) < self.epsilon\n",
    "        \n",
    "        if do_exploration\n",
    "            action = np.random[:randint](1, self.action_size + 1)\n",
    "            \n",
    "        else\n",
    "            action = greedy_action\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    self.last_state = state\n",
    "    self.last_action = action\n",
    "    return action\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aqui creamos nuestra tabla Tileada, utilizando solamente 5 bins por cada estado parametro que hay y las tres acciones que ya conocemos del acrobot dandonos una tabla q de 5x5x5x5x5x5x3\n",
    "dado esto podemos crear nuestro nuevo agente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.0, 1.0]/10 + (-0.13333334028720856) => [-0.933333, -0.733333, -0.533333, -0.333333, -0.133333, 0.0666667, 0.266667, 0.466667, 0.666667]\n",
      "    [-1.0, 1.0]/10 + (-0.13333334028720856) => [-0.933333, -0.733333, -0.533333, -0.333333, -0.133333, 0.0666667, 0.266667, 0.466667, 0.666667]\n",
      "    [-1.0, 1.0]/10 + (-0.13333334028720856) => [-0.933333, -0.733333, -0.533333, -0.333333, -0.133333, 0.0666667, 0.266667, 0.466667, 0.666667]\n",
      "    [-1.0, 1.0]/10 + (-0.13333334028720856) => [-0.933333, -0.733333, -0.533333, -0.333333, -0.133333, 0.0666667, 0.266667, 0.466667, 0.666667]\n",
      "    [-12.566370964050293, 12.566370964050293]/10 + (-1.675516128540039) => [-11.7286, -9.21534, -6.70206, -4.18879, -1.67552, 0.837758, 3.35103, 5.86431, 8.37758]\n",
      "    [-28.274333953857422, 28.274333953857422]/10 + (-3.769911289215088) => [-26.3894, -20.7345, -15.0796, -9.42478, -3.76991, 1.88496, 7.53982, 13.1947, 18.8496]\n",
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.0, 1.0]/10 + (0.0) => [-0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8]\n",
      "    [-1.0, 1.0]/10 + (0.0) => [-0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8]\n",
      "    [-1.0, 1.0]/10 + (0.0) => [-0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8]\n",
      "    [-1.0, 1.0]/10 + (0.0) => [-0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8]\n",
      "    [-12.566370964050293, 12.566370964050293]/10 + (0.0) => [-10.0531, -7.53982, -5.02655, -2.51327, 0.0, 2.51327, 5.02655, 7.53982, 10.0531]\n",
      "    [-28.274333953857422, 28.274333953857422]/10 + (0.0) => [-22.6195, -16.9646, -11.3097, -5.65487, 0.0, 5.65487, 11.3097, 16.9646, 22.6195]\n",
      "Tiling: [<low>, <high>] / <bins> + (<offset>) => <splits>\n",
      "    [-1.0, 1.0]/10 + (0.13333334028720856) => [-0.666667, -0.466667, -0.266667, -0.0666667, 0.133333, 0.333333, 0.533333, 0.733333, 0.933333]\n",
      "    [-1.0, 1.0]/10 + (0.13333334028720856) => [-0.666667, -0.466667, -0.266667, -0.0666667, 0.133333, 0.333333, 0.533333, 0.733333, 0.933333]\n",
      "    [-1.0, 1.0]/10 + (0.13333334028720856) => [-0.666667, -0.466667, -0.266667, -0.0666667, 0.133333, 0.333333, 0.533333, 0.733333, 0.933333]\n",
      "    [-1.0, 1.0]/10 + (0.13333334028720856) => [-0.666667, -0.466667, -0.266667, -0.0666667, 0.133333, 0.333333, 0.533333, 0.733333, 0.933333]\n",
      "    [-12.566370964050293, 12.566370964050293]/10 + (1.675516128540039) => [-8.37758, -5.86431, -3.35103, -0.837758, 1.67552, 4.18879, 6.70206, 9.21534, 11.7286]\n",
      "    [-28.274333953857422, 28.274333953857422]/10 + (3.769911289215088) => [-18.8496, -13.1947, -7.53982, -1.88496, 3.76991, 9.42478, 15.0796, 20.7345, 26.3894]\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "QTable(): size = (10, 10, 10, 10, 10, 10, 3)\n",
      "TiledQTable(): no. of internal tables = 3\n",
      "Environment: PyObject <TimeLimit<AcrobotEnv<Acrobot-v1>>>\n",
      "State space sizes: NTuple{6,Int64}[(10, 10, 10, 10, 10, 10), (10, 10, 10, 10, 10, 10), (10, 10, 10, 10, 10, 10)]\n",
      "Action space size: 3\n"
     ]
    }
   ],
   "source": [
    "n_bins = 10\n",
    "\n",
    "bins = tuple([n_bins for i in 1:env[:observation_space][:shape][1]]...)\n",
    "\n",
    "\n",
    "\n",
    "offset_pos = (env[:observation_space][:high] - env[:observation_space][:low])/15\n",
    "\n",
    "\n",
    "offset_pos = convert(Array{Float64,1}, offset_pos)\n",
    "\n",
    "tiling_specs = [(bins, -offset_pos), \n",
    "                (bins, tuple([0.0 for i in 1:env[:observation_space][:shape][1]]...)),\n",
    "                (bins, offset_pos)]\n",
    "\n",
    "\n",
    "tq = TiledQTable(env[:observation_space][:low], \n",
    "                 env[:observation_space][:high], \n",
    "                 tiling_specs, \n",
    "                 env[:action_space][:n])\n",
    "\n",
    "\n",
    "agent = QLearningAgent(env, tq);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora nomas creamos una funcion que utilice nuestro modelo de q learning y aprenda, dado esto guardamos los scores y decidimos el numero de episodios que queremos que juegue nuestro robotsillo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " Episode 100/10000 | reward: -500.0\r",
      " Episode 200/10000 | reward: -500.0"
     ]
    }
   ],
   "source": [
    "function run(agent, env, num_episodes = 10000, mode = \"train\")                 \n",
    "    scores = []\n",
    "    for i_episode in 1:num_episodes\n",
    "        state = env[:reset]()\n",
    "        action = reset_episode(agent,state)\n",
    "        total_reward = 0\n",
    "        done = false\n",
    "        \n",
    "        while !done\n",
    "            state, reward, done, info = env[:step](action - 1)\n",
    "            total_reward += reward\n",
    "            action = act(agent,state,reward,mode)\n",
    "        end\n",
    "        append!(scores, total_reward)\n",
    "        if i_episode % 100 == 0\n",
    "            print(\"\\r Episode $i_episode/$num_episodes | reward: $total_reward\")\n",
    "        end\n",
    "    end\n",
    "    return scores\n",
    "end\n",
    "scores = run(agent, env);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 / 1000 | Max Average Score: -783.0 /r"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "state = env[:reset]()\n",
    "score = 0\n",
    "rewards = []\n",
    "action = act(agent, state, 0, \"test\")\n",
    "for t in 1:1000\n",
    "    env[:render]()\n",
    "    state, reward, done, _ = env[:step](action-1)\n",
    "    push!(rewards,reward)\n",
    "    action = act(agent, state, 0, \"test\")\n",
    "    score += reward\n",
    "    if t % 100 == 0\n",
    "        print(\"\\rEpisode $t / 1000 | Max Average Score: $score /r\")\n",
    "    end\n",
    "end\n",
    "env[:close]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 / 1000 | Max Average Score: -1000.0"
     ]
    }
   ],
   "source": [
    "state = env[:reset]()\n",
    "score = 0\n",
    "dumbrewards = []\n",
    "action = env[:action_space][:sample]()\n",
    "for t in 1:1000\n",
    "    env[:render]()\n",
    "    state, reward, done, _ = env[:step](action)\n",
    "    push!(dumbrewards, reward)\n",
    "    action = env[:action_space][:sample]()\n",
    "    score += reward\n",
    "    if t%100 ==0\n",
    "        print(\"\\rEpisode $t / 1000 | Max Average Score: $score\")\n",
    "    end\n",
    "end\n",
    "env[:close]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
